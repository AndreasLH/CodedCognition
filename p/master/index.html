<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Master's Thesis"><title>Weakly Supervised 3D Object Detection</title>
<link rel=canonical href=https://andreaslh.github.io/CodedCognition/p/master/><link rel=stylesheet href=/CodedCognition/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Weakly Supervised 3D Object Detection"><meta property='og:description' content="Master's Thesis"><meta property='og:url' content='https://andreaslh.github.io/CodedCognition/p/master/'><meta property='og:site_name' content='Coded Cognition // Andreas Lau Hansen'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='3D'><meta property='article:tag' content='Computer vision'><meta property='article:tag' content='CNN'><meta property='article:published_time' content='2024-08-05T12:00:00+00:00'><meta property='article:modified_time' content='2024-08-05T12:00:00+00:00'><meta property='og:image' content='https://andreaslh.github.io/CodedCognition/p/master/preds.png'><meta name=twitter:title content="Weakly Supervised 3D Object Detection"><meta name=twitter:description content="Master's Thesis"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://andreaslh.github.io/CodedCognition/p/master/preds.png'><link rel="shortcut icon" href=/CodedCognition/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/CodedCognition/><img src=/CodedCognition/img/avatar_hu18252397595957545659.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/CodedCognition>Coded Cognition // Andreas Lau Hansen</a></h1><h2 class=site-description>My personal website. I want to show some of the things I have worked on and that I have found particularly interesting during my studies in AI and Data at DTU</h2></div></header><ol class=menu-social><li><a href=https://github.com/andreasLH target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/andreas-lau-hansen-332962264/ target=_blank title=LinkedIn rel=me><svg xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 256 256" width="50" height="50" fill-rule="nonzero"><g fill="#707070" fill-rule="nonzero" stroke="none" stroke-width="1" stroke-linecap="butt" stroke-linejoin="miter" stroke-miterlimit="10" stroke-dasharray="" stroke-dashoffset="0" font-family="none" font-weight="none" font-size="none" text-anchor="none" style="mix-blend-mode:normal"><g transform="scale(5.12,5.12)"><path d="M9 4C6.25048 4 4 6.25048 4 9v32c0 2.74952 2.25048 5 5 5h32c2.74952.0 5-2.25048 5-5V9c0-2.74952-2.25048-5-5-5zM9 6h32c1.66848.0 3 1.33152 3 3v32c0 1.66848-1.33152 3-3 3H9c-1.66848.0-3-1.33152-3-3V9c0-1.66848 1.33152-3 3-3zm5 5.01172c-1.09522.0-2.08078.32736-2.81055.94141-.72977.61405-1.17773 1.53139-1.17773 2.51367.0 1.86718 1.61957 3.32281 3.67969 3.4668.0013 65e-5.0026.0013.00391.00195.09817.03346.20099.05126.30469.05273 2.27301.0 3.98828-1.5922 3.98828-3.52148-18e-5-.01759-83e-5-.03518-.00195-.05274-.10175-1.90023-1.79589-3.40234-3.98633-3.40234zm0 1.97656c1.39223.0 1.94197.62176 2.00195 1.50391-.01215.85625-.54186 1.51953-2.00195 1.51953-1.38541.0-2.01172-.70949-2.01172-1.54492.0-.41771.15242-.7325.47266-1.00195.32023-.26945.83428-.47656 1.53906-.47656zM11 19c-.55226 6e-5-.99994.44774-1 1v19c6e-5.55226.44774.99994 1 1h6c.55226-6e-5.99994-.44774 1-1v-5.86523V20c-6e-5-.55226-.44774-.99994-1-1zm9 0c-.55226 6e-5-.99994.44774-1 1v19c6e-5.55226.44774.99994 1 1h6c.55226-6e-5.99994-.44774 1-1V29c0-.82967.22639-1.65497.625-2.19531.39861-.54035.90147-.86463 1.85742-.84766.98574.01695 1.50758.35464 1.90234.88477.39477.53013.61523 1.32487.61523 2.1582v10c6e-5.55226.44774.99994 1 1h6c.55226-6e-5.99994-.44774 1-1V28.26172c0-2.96154-.87721-5.30739-2.38086-6.89453C36.11548 19.78005 34.02416 19 31.81249 19c-2.10202.0-3.70165.70489-4.8125 1.42383V20c-6e-5-.55226-.44774-.99994-1-1zm-8 2h4v12.13477V38h-4zm9 0h4v1.56055c13e-5.43.27511.81179.68291.94817.40781.13638.85714-.00319 1.11591-.34661.0.0 1.57037-2.16211 5.01367-2.16211 1.75333.0 3.25687.58258 4.35547 1.74219s1.83203 2.94607 1.83203 5.51953V38h-4v-9c0-1.16667-.27953-2.37289-1.00977-3.35352-.73023-.98062-1.9584-1.66341-3.47266-1.68945-1.52204-.02703-2.77006.66996-3.50195 1.66211S24.99999 27.82967 24.99999 29v9h-4z"/></g></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/CodedCognition/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/CodedCognition/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/CodedCognition/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/CodedCognition/about-me/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About me</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#data-annotation-is-difficult>Data annotation is difficult</a></li><li><a href=#the-problem-is-more-difficult>The problem is more difficult</a></li></ol><ol><li><a href=#metrics>Metrics</a></li><li><a href=#pictures>Pictures</a><ol><li><a href=#breaking-the-model>Breaking the model</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/CodedCognition/p/master/><img src=/CodedCognition/p/master/preds_hu12753443730573609829.png srcset="/CodedCognition/p/master/preds_hu12753443730573609829.png 800w, /CodedCognition/p/master/preds_hu12813807622330571018.png 1600w" width=800 height=696 loading=lazy alt="Featured image of post Weakly Supervised 3D Object Detection"></a></div><div class=article-details><header class=article-category><a href=/CodedCognition/categories/thesis/ style=background-color:#f88167;color:#fff>Thesis</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/CodedCognition/p/master/>Weakly Supervised 3D Object Detection</a></h2><h3 class=article-subtitle>Master's Thesis</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 05, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>5 minute read</time></div></footer></div></header><section class=article-content><h1 id=introduction>Introduction</h1><p>2D object detection has for some time been quite successful put simply it can tell where in the image an object sits, which has many use cases. However to place an object in physical dimensions 3D object detection is needed. The figure below shows the natural extension.</p><figure><img src=/CodedCognition/p/master/motivation.png width=80%><figcaption><h4>Why 3D object detection is more useful. The squares in the rightmost figure are 1mx1m in size.</h4></figcaption></figure><h2 id=data-annotation-is-difficult>Data annotation is difficult</h2><p>However, one of the biggest challenges with 3D object detection is that large datasets are not widely available. This figure quite nicely illustrates one the reasons why that is - the data rougly takes 11x more time to annotate, and that is not counting how much more difficult it is to collect the data. Collection typically requires complicated setups with depth sensing cameras or whole room scanning.<figure><img src=/CodedCognition/p/master/annotation.png width=80%><figcaption><h4>Annotation time motivation.</h4></figcaption></figure></p><p>This begs the question:
<strong>How to make a 3D object detection model using only 2D annotation boxes</strong></p><p>So we&rsquo;re not interested in making the best possible 3D object detector, but one that is efficient with its use of data. That is why we call it weakly supervised 3D object detection.</p><h2 id=the-problem-is-more-difficult>The problem is more difficult</h2><p>This project only concerns using a single camera, so-called monocular object detection. This makes this model much more accessible, as anyone with a simple camera for example a phone can use the model to generate predictions. An issue with this problem is that it makes the problem much more difficult, to determine the physical location of an object one requires at least 2 distinct view points. Without this, you have to resort to estimation.</p><figure><img src=/CodedCognition/p/master/line.png width=50%><figcaption><h4>A point in an image, corresponds to an infinite number of points in 3D.</h4></figcaption></figure><p>The figure shows how estimating the depth is the biggest challenge, because a large object far away visually appears the same size as a smaller object close by.</p><h1 id=approach-weak-cube-r-cnn>Approach: &ldquo;Weak Cube R-CNN&rdquo;</h1><p>We take heavy inspiration from another model <a class=link href=https://arxiv.org/abs/2207.10660 target=_blank rel=noopener>Cube R-CNN</a>, but remove all the components related to its 3D detection capability. As such what remains is essentially a Faster R-CNN model. Our model predicts 3D bounding boxes from the 2D predicted bounding boxes.</p><p>The crucial component is how to introduce depth sensing, since we do not have the 3D annotations anymore. To this end we use a depth estimation &ldquo;Depth Anything&rdquo; model tuned for metric depth estimation (estimating depth in meters), with this model you can effectively get a pseudo ground truth of the depth of the scene. The figure below shows how this might look for an image.</p><figure><img src=/CodedCognition/p/master/depth_map.png width=80%><figcaption><h4>Depth map.</h4></figcaption></figure><p>Interestingly, one can also interpret the depth map as a point-cloud by transforming it with simple math. This proves quite useful, because one can use it for other downstream tasks.<figure><img src=/CodedCognition/p/master/point_cloud_floor.png width=60%><figcaption><h4>Point cloud generated from a depth map.</h4></figcaption></figure></p><p>When you have the point-cloud it is possible to estimate the ground plane. This is useful because the rotation, along with the depth, is one of the most important characteristics of getting high accuracy. Additionally, it utilises the frame of reference that is present in scenes. However, running a ground estimation algorithm on the point cloud directly is highly unstable, as there are typically other more dominant planes in a scene, like the walls. We overcome this by using a combination of the GroundingDINO and Segment Anything methods (middle image), this effectively filters the ground. Now you can run a simple plane-estimation RANSAC algorithm on the filtered point cloud, this turns out to be quite robust.<figure><img src=/CodedCognition/p/master/rotation.png width=90%><figcaption><h4>How you can obtain a ground normal from the estimated point cloud.</h4></figcaption></figure></p><p>Based on all of these properties obtained from the scene, we build a variety of loss functions to optimise the model. A large overview of the whole model is shown.<figure><img src=/CodedCognition/p/master/model_arch-loss_vertical-3.svg width=95%><figcaption><h4>Overview of the whole model</h4></figcaption></figure></p><p>In total the whole optimisation objective for the 3D part of the model becomes:
$$
L_{3D} = L_{GIoU} + L_z + L_{dim} + L_{range} + L_{normal} + L_{pose}
$$</p><p>So we have something for the positioning (first 2), the dimension (middle 2), and the rotation (last 2).
These losses are clearly the most important part of the model. Additionally, each term is regularised by a $\lambda$.</p><h1 id=results>Results</h1><p>For reference to our method we also intially developed a &ldquo;proposal method&rdquo; based on the idea of proposing many cubes and selecting the best ones. The ideas generated when developing this method were carried over to our learnable <em>Weak Cube R-CNN</em> method.</p><h2 id=metrics>Metrics</h2><p>The notable thing to report on the developed method, is that it achieves higher accuracy than a corresponding fully supervised but annotation time-equalised model.<figure><img src=/CodedCognition/p/master/result.png width=90%><figcaption><h4>Results on the chosen metric Average precision 2D and 3D</h4></figcaption></figure></p><p>The AP numbers might not look that impressive, but the following figure puts them into context. The AP3D metric is very punishing when just slightly off.<figure><img src=/CodedCognition/p/master/3diou.png width=90%><figcaption><h4>Achieving a high 3D IoU is really hard.</h4></figcaption></figure></p><h2 id=pictures>Pictures</h2><p>What we are here for!!<figure><img src=/CodedCognition/p/master/weak.png width=95%><figcaption><h4>Results from the Weak-Cube R-CNN method</h4></figcaption></figure><figure><img src=/CodedCognition/p/master/time-eq.png width=95%><figcaption><h4>Results from the Annotation time equalised Cube R-CNN method</h4></figcaption></figure><figure><img src=/CodedCognition/p/master/preds.png width=95%><figcaption><h4>Results from the standard Cube R-CNN method</h4></figcaption></figure></p><h3 id=breaking-the-model>Breaking the model</h3><p>Just trying random stuff, that is really out of domain for the model to see what happens.</p><figure><img src=/CodedCognition/p/master/fun.png width=95%><figcaption><h4>Left: The model predicting wrongly on illusions. Right: When you provide a fake depth map of the same depth, it still looks correct from the front</h4></figcaption></figure><h1 id=huggingface-demo>Huggingface demo</h1><p>If you want to try the model yourself, we are hosting a demo version on <a class=link href=https://huggingface.co/spaces/AndreasLH/Weakly-Supervised-3DOD target=_blank rel=noopener>Huggingface</a>.</p><div style=text-align:center><iframe src=https://andreaslh-weakly-supervised-3dod.hf.space frameborder=0 width=90% height=1400></iframe></div></section><footer class=article-footer><section class=article-tags><a href=/CodedCognition/tags/3d/>3D</a>
<a href=/CodedCognition/tags/computer-vision/>Computer Vision</a>
<a href=/CodedCognition/tags/cnn/>CNN</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Aug 05, 2024 12:00 UTC</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/CodedCognition/p/heart-rhythm-estimation-in-face-videos/><div class=article-image><img src=/CodedCognition/p/heart-rhythm-estimation-in-face-videos/bvp2.4b4356fc54af5b01e8f82d57119340c9_hu9806043450939045086.png width=250 height=150 loading=lazy alt="Featured image of post Heart rhythm estimation in face videos" data-hash="md5-S0NW/FSvWwHo+C1XEZNAyQ=="></div><div class=article-details><h2 class=article-title>Heart rhythm estimation in face videos</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2019 -
2024 Coded Cognition // Andreas Lau Hansen</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/CodedCognition/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>