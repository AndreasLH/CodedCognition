[{"content":"Introduction 2D object detection has for some time been quite successful put simply it can tell where in the image an object sits, which has many use cases. However to place an object in physical dimensions 3D object detection is needed. The figure below shows the natural extension.\nWhy 3D object detection is more useful. The squares in the rightmost figure are 1mx1m in size. Data annotation is difficult However, one of the biggest challenges with 3D object detection is that large datasets are not widely available. This figure quite nicely illustrates one the reasons why that is - the data rougly takes 11x more time to annotate, and that is not counting how much more difficult it is to collect the data. Collection typically requires complicated setups with depth sensing cameras or whole room scanning. Annotation time motivation. This begs the question: How to make a 3D object detection model using only 2D annotation boxes\nSo we\u0026rsquo;re not interested in making the best possible 3D object detector, but one that is efficient with its use of data. That is why we call it weakly supervised 3D object detection.\nThe problem is more difficult This project only concerns using a single camera, so-called monocular object detection. This makes this model much more accessible, as anyone with a simple camera for example a phone can use the model to generate predictions. An issue with this problem is that it makes the problem much more difficult, to determine the physical location of an object one requires at least 2 distinct view points. Without this, you have to resort to estimation.\nA point in an image, corresponds to an infinite number of points in 3D. The figure shows how estimating the depth is the biggest challenge, because a large object far away visually appears the same size as a smaller object close by.\nApproach: \u0026ldquo;Weak Cube R-CNN\u0026rdquo; We take heavy inspiration from another model Cube R-CNN, but remove all the components related to its 3D detection capability. As such what remains is essentially a Faster R-CNN model. Our model predicts 3D bounding boxes from the 2D predicted bounding boxes.\nThe crucial component is how to introduce depth sensing, since we do not have the 3D annotations anymore. To this end we use a depth estimation \u0026ldquo;Depth Anything\u0026rdquo; model tuned for metric depth estimation (estimating depth in meters), with this model you can effectively get a pseudo ground truth of the depth of the scene. The figure below shows how this might look for an image.\nDepth map. Interestingly, one can also interpret the depth map as a point-cloud by transforming it with simple math. This proves quite useful, because one can use it for other downstream tasks. Point cloud generated from a depth map. When you have the point-cloud it is possible to estimate the ground plane. This is useful because the rotation, along with the depth, is one of the most important characteristics of getting high accuracy. Additionally, it utilises the frame of reference that is present in scenes. However, running a ground estimation algorithm on the point cloud directly is highly unstable, as there are typically other more dominant planes in a scene, like the walls. We overcome this by using a combination of the GroundingDINO and Segment Anything methods (middle image), this effectively filters the ground. Now you can run a simple plane-estimation RANSAC algorithm on the filtered point cloud, this turns out to be quite robust. How you can obtain a ground normal from the estimated point cloud. Based on all of these properties obtained from the scene, we build a variety of loss functions to optimise the model. A large overview of the whole model is shown. Overview of the whole model In total the whole optimisation objective for the 3D part of the model becomes: $$ L_{3D} = L_{GIoU} + L_z + L_{dim} + L_{range} + L_{normal} + L_{pose} $$\nSo we have something for the positioning (first 2), the dimension (middle 2), and the rotation (last 2). These losses are clearly the most important part of the model. Additionally, each term is regularised by a $\\lambda$.\nResults For reference to our method we also intially developed a \u0026ldquo;proposal method\u0026rdquo; based on the idea of proposing many cubes and selecting the best ones. The ideas generated when developing this method were carried over to our learnable Weak Cube R-CNN method.\nMetrics The notable thing to report on the developed method, is that it achieves higher accuracy than a corresponding fully supervised but annotation time-equalised model. Results on the chosen metric Average precision 2D and 3D The AP numbers might not look that impressive, but the following figure puts them into context. The AP3D metric is very punishing when just slightly off. Achieving a high 3D IoU is really hard. Pictures What we are here for!! Results from the Weak-Cube R-CNN method Results from the Annotation time equalised Cube R-CNN method Results from the standard Cube R-CNN method Breaking the model Just trying random stuff, that is really out of domain for the model to see what happens.\nLeft: The model predicting wrongly on illusions. Right: When you provide a fake depth map of the same depth, it still looks correct from the front Huggingface demo If you want to try the model yourself, we are hosting a demo version on Huggingface.\n","date":"2024-08-05T12:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/master/preds_hu15044027111139814822.png","permalink":"https://andreaslh.github.io/CodedCognition/p/master/","title":"Weakly Supervised 3D Object Detection"},{"content":"Motivation I realised midways during this project that we had essentially just recreated the CLIP model.\nData Contrastive learning Math $$ L_{bi} (a^{n=i}, b^{n=i}, b^{b\\neq i}, a^{n\\neq i}) = \\frac{1}{B} \\sum_{j=1}^B L^\\prime_{bi}(i,j)\\delta(i,j) $$\nModel Results We can go both forwards and backwards between images and text.\nOut of distribution image ","date":"2023-11-26T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/hello-world/cover2_hu12110017146130817782.png","permalink":"https://andreaslh.github.io/CodedCognition/p/hello-world/","title":"Image to recipe retrieval"},{"content":"CPU Efficient computation and hardware utilisation underpins most of the methods used in deep learning learning applications.\nDifferent implementations of matrix multiplication on cpu Why it is faster to access in the mkn pattern Making things faster: multithreading For this, we are chaning the problem to model the heat spread in a room\nRoom shown at different times Now we can utilise all the cores in the machine to make the simulation go a lot faster.\nPerformance when using multiple cores, measured in Mega lattice updates/s Making things even faster: using a gpu The idea of using blocking is to allow each thread to work on more than one element of the C matrix before being released. This might improve performance, because the overhead of assigning new threads to calculate each element is higher than assigning new work to each thread.\nDifferent implementations of matrix multiplication on cpu ","date":"2024-01-15T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/efficient-use-of-computation/cpu_hu14396667161613057915.png","permalink":"https://andreaslh.github.io/CodedCognition/p/efficient-use-of-computation/","title":"Efficient use of computation"},{"content":"","date":"2023-10-15T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/linear-encoding-of-faces/generated_hu11069046077238625139.png","permalink":"https://andreaslh.github.io/CodedCognition/p/linear-encoding-of-faces/","title":"Linear Encoding of faces"},{"content":"Architecture Using the trusty ResNet18 backbone\nResults ","date":"2023-06-21T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/trash-detection/out_hu10899831072320457152.png","permalink":"https://andreaslh.github.io/CodedCognition/p/trash-detection/","title":"Trash Detection"},{"content":"","date":"2023-06-16T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/gan-latent-space/directions_hu3149539926568328651.png","permalink":"https://andreaslh.github.io/CodedCognition/p/gan-latent-space/","title":"GAN Latent space"},{"content":"\n","date":"2023-06-12T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/semantic-image-segmentation/preds_hu11129798262431947290.png","permalink":"https://andreaslh.github.io/CodedCognition/p/semantic-image-segmentation/","title":"Semantic Image Segmentation"},{"content":"Class Activation mappings (CAM) based on the output of a simple ResNet18 model Prototype based network. Works on a this-looks-like-that principle, i.e. it looks in other images with similar features to explain why it classifies image to certain classes. ","date":"2023-04-05T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/explainable-ai-using-saliency-maps-and-proto-pnet/image_hu2486855072546096916.png","permalink":"https://andreaslh.github.io/CodedCognition/p/explainable-ai-using-saliency-maps-and-proto-pnet/","title":"Explainable AI using Saliency maps and Proto-PNet"},{"content":"This was mostly about learning the whole MLOps pipeline, to train, push monitor and so on, the machine learning life cycle.\n","date":"2023-01-19T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/mlops-for-butterfly-image-generation/api_generations_hu11836242087022802759.png","permalink":"https://andreaslh.github.io/CodedCognition/p/mlops-for-butterfly-image-generation/","title":"MLOps for Butterfly image generation"},{"content":" Processing the data from the BVP sensor Use the excellent MediaPipe framework to find faces in videos. The video below shows how excellent this library is at finding the face.\nExtraction of the skin of the face, and finding the relevant keypoints. ","date":"2022-06-06T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/heart-rhythm-estimation-in-face-videos/bvp2_hu18265746848985410586.png","permalink":"https://andreaslh.github.io/CodedCognition/p/heart-rhythm-estimation-in-face-videos/","title":"Heart rhythm estimation in face videos"},{"content":"\n","date":"2022-05-15T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/multiagent-planning/pepper_hu15347806128151614641.png","permalink":"https://andreaslh.github.io/CodedCognition/p/multiagent-planning/","title":"Multiagent planning"},{"content":"Scraping wikipedia\nWe made this as its own separate website.\n","date":"2022-05-10T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/the-social-network-of-hip-hop-artists/community_hu5237257608381400938.png","permalink":"https://andreaslh.github.io/CodedCognition/p/the-social-network-of-hip-hop-artists/","title":"The social network of Hip-Hop artists"},{"content":"","date":"2021-12-10T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/image-colourisation/preds_hu13378546266269235231.png","permalink":"https://andreaslh.github.io/CodedCognition/p/image-colourisation/","title":"Image colourisation"},{"content":"","date":"2021-06-12T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/jewellery-recommendation-for-e-commerce/VAE_hu7910313137396713631.png","permalink":"https://andreaslh.github.io/CodedCognition/p/jewellery-recommendation-for-e-commerce/","title":"Jewellery recommendation for e-commerce"},{"content":" Snapshot of the environment, we are driving the green car Model the environment as a time to collision matrix.\nTime to collision (TTC) matrix 5 available actions A(s), which are: {0: ‘LANE_LEFT’, 1: ‘IDLE’, 2: ‘LANE_RIGHT’, 3: ‘FASTER’, 4: ‘SLOWER’}\n","date":"2021-05-07T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/route-planning-for-self-driving-cars/car_hu4401781279362557537.png","permalink":"https://andreaslh.github.io/CodedCognition/p/route-planning-for-self-driving-cars/","title":"Route Planning for self driving cars"},{"content":"The Fashion-MNIST data-set is used in this project. It is a collection of Zalando’s article images consisting of 60.000 training points and 10.000 test points. The data-set is balanced perfectly so that each class has 6.000 training and 1.000 test points. Each data-point resembles a grey-scale image of size 28x28 pixels, with a label indicating which class it belongs to (a total of 10 classes).\nWe used this dataset as it is sufficiently simple but still complicated enough that a model won\u0026rsquo;t get 100% accuracy\nDifferent uncertainy sampling strategies\nLeast confident\n$$ x^{\\ast} = \\argmax_{x \\in U} 1 − p_{\\theta} \\left( y_{(1)}|x \\right) $$ maximum margin $$ x^{\\ast} = \\argmax_{x \\in U} p_{\\theta} \\left( y_{(1)}|x \\right) − p_{\\theta} \\left( y_{(2)}|x \\right) $$ entropy $$ x^{\\ast} = \\argmax_{x \\in U} -\\sum_{y-\\in Y} p_{\\theta} \\left( y|x \\right) \\log p_\\theta (y|x) $$\n","date":"2021-04-08T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/active-learning/image_hu6193538363863112961.png","permalink":"https://andreaslh.github.io/CodedCognition/p/active-learning/","title":"Active learning"},{"content":"We made a phone app and a watch app, among some other things. The watch app is below.\nYou can try them out and click around.\nThe watch app ","date":"2020-12-07T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/diabetes-assistant/cover_hu10675163458533183498.png","permalink":"https://andreaslh.github.io/CodedCognition/p/diabetes-assistant/","title":"Diabetes Assistant"},{"content":"fastText and GloVe embedding methods\nPCA for visualisation. The first of many times I have used PCA.\n","date":"2020-05-12T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/text-classification-by-word-embeddings/image_hu6356095866655738112.png","permalink":"https://andreaslh.github.io/CodedCognition/p/text-classification-by-word-embeddings/","title":"Text classification by word embeddings"},{"content":"In this small project we made a classification system for emails into spam and regular emails. We used relatively simple methods for converting the text into vectors and classification, however it turned out to be rather effective. Even with these very simple methods it is possible to get an accuracy \u0026gt; 95%. Although this particular problem is not the most difficult, as we are only concerned with a binary classification.\nThe general outline was to build a huge sparse matrix matrix based on the counts of words in each document (email), think something like $n_{docs} \\times n_{words}$ of the order $4000 \\times 25000$.\nUsing the TF-IDF measure downweighs the longer documents.\nWe get a large matrix that looks something like this.\nword 1 word 2 \u0026hellip; \u0026hellip; word n Document 1 tf-idf(word 1) tfidf(word 2) \u0026hellip; \u0026hellip; tfidf(word n) Document 2 tf-idf(word 1) tfidf(word 2) \u0026hellip; \u0026hellip; tfidf(word n) \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Document n tf-idf(word 1) tfidf(word 2) \u0026hellip; \u0026hellip; tfidf(word n) It\u0026rsquo;s quite important to represent it as sparse matrix as it is otherwise quite bothersome to such a large matrix. Furthermore, it is largely filled with 0\u0026rsquo;s.\nThen we can build a classifier based on the naive bayes framework.\n$$ P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) $$ $$ \\Downarrow $$ $$ \\hat{y} = \\argmax_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y) $$\nwhere\n$$ P(x_{i}|y) = \\hat{\\theta_{yi}} = \\frac{N_{yi} + \\alpha}{N_{y} + \\alpha n} $$\nEvaluation We can examine how the different methods stack up against each other\nAccuracy of each method, including a few other baseline methods some of which use a simpler word vectorisation method, bag-of-words, of counting So, we can build a relatively simple classifier that can obtain a \u0026gt;95% accuracy.\nIt\u0026#39;s also interesting to examine which classes are most often classified wrongly and in which way. This figure shows this relationship ","date":"2020-01-15T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/ham-or-spam/cover_hu17385296966383030039.png","permalink":"https://andreaslh.github.io/CodedCognition/p/ham-or-spam/","title":"Ham or spam"},{"content":"This was the very first time I played around with an image classifier, just a week after starting my Bachelor\u0026rsquo;s degree in Artificial intelligence and Data. Back then I had no idea how the model actually worked, but accepted is as a black box.\nBasically the experiment was to use an out-of-the box Resnet50 model to recognise a phone at different angles. Another purpose of the experiment was to get familiar with means and confidence intervals.\nThis was gathered in a table\nAngle (deg.) Avg. (%) 90 y y y y y y 100 80 y y y y y y 100 70 y y y y y y 100 60 y y y y y y 100 50 y y y y y y 100 40 y y y y y y 100 30 y y y y y y 100 20 y y y y y y 100 10 n n n y y n 33.33 0 n y n n n n 16.7 We collected these results and found an average accuracy of 85% with a confidence interval [80.6%; 89.4%].\nAll this was as simple as the following code and some boilerplate to load the image\n1 2 3 4 5 6 model = resnet50.ResNet50(weights=\u0026#39;imagenet\u0026#39;) # load the image # .... processed_image = resnet50.preprocess_input(np.expand_dims(frame, axis=0)) predictions = model.predict(processed_image) label = resnet50.decode_predictions(predictions) ","date":"2019-09-17T00:00:00Z","image":"https://andreaslh.github.io/CodedCognition/p/object-recognition-at-different-angles/angle%20experiment_hu5945757808037544276.png","permalink":"https://andreaslh.github.io/CodedCognition/p/object-recognition-at-different-angles/","title":"Object recognition at different angles"}]